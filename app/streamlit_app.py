import os
import pandas as pd
import numpy as np
import re
import pickle
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import streamlit as st
from matplotlib import font_manager, rc

# âœ… ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "..", "data", "preprocessed_labeled.csv")
MODEL_PATH = os.path.join(BASE_DIR, "..", "model", "sentiment_model.keras")
TOKENIZER_PATH = os.path.join(BASE_DIR, "..", "model", "tokenizer.pkl")
FONT_PATH = os.path.join(BASE_DIR, "..", "data", "NanumGothic.ttf")

# âœ… í°íŠ¸ ì„¤ì •
font_name = font_manager.FontProperties(fname=FONT_PATH).get_name()
rc('font', family=font_name)
plt.rcParams['font.family'] = font_name  # WordCloud í•œê¸€ê¹¨ì§ ë°©ì§€

# âœ… Streamlit UI ì„¤ì •
st.set_page_config(page_title="ë¶€ë™ì‚° ë‰´ìŠ¤ ê°ì • ë¶„ì„", layout="wide")
st.markdown(f"<h1 style='text-align:center; color:#1A535C;'>ğŸ  ë¶€ë™ì‚° ë‰´ìŠ¤ ê°ì • ë¶„ì„ ëŒ€ì‹œë³´ë“œ</h1>", unsafe_allow_html=True)
st.markdown("<hr style='border: 1px solid #eee;'>", unsafe_allow_html=True)

# âœ… ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë”©
@st.cache_resource
def load_resources():
    model = load_model(MODEL_PATH)
    with open(TOKENIZER_PATH, "rb") as f:
        tokenizer = pickle.load(f)
    return model, tokenizer

model, tokenizer = load_resources()

# âœ… ì „ì²˜ë¦¬ í•¨ìˆ˜
stopwords = ["ì˜", "ê°€", "ì´", "ì€", "ë“¤", "ëŠ”", "ì¢€", "ì˜", "ê±", "ê³¼", "ë„", "ë¥¼", "ìœ¼ë¡œ", "ì", "ì—", "ì™€", "í•œ", "í•˜ë‹¤"]

def clean_text(text):
    text = re.sub(r"[^ê°€-í£0-9\s]", "", str(text))
    return re.sub(r"\s+", " ", text).strip()

def tokenize(text):
    return [word for word in clean_text(text).split() if word not in stopwords]

def preprocess(text):
    tokens = tokenize(text)
    seq = tokenizer.texts_to_sequences([" ".join(tokens)])
    return pad_sequences(seq, maxlen=30)

# âœ… ë°ì´í„° ë¡œë”©
@st.cache_data
def load_dataframe():
    if os.path.exists(DATA_PATH):
        df = pd.read_csv(DATA_PATH)
        return df.dropna(subset=["clean_title", "label"])
    return pd.DataFrame(columns=["clean_title", "label"])

if "df" not in st.session_state:
    st.session_state.df = load_dataframe()

df = st.session_state.df

# âœ… ìˆ˜ë™ ìƒ˜í”Œ ì¶”ê°€
manual = pd.DataFrame([
    {"clean_title": "ì„œìš¸ ì•„íŒŒíŠ¸ ë¶„ì–‘ ì™„íŒ, ì‹¤ìˆ˜ìš”ì ëª°ë ¤", "label": 1},
    {"clean_title": "ì •ë¶€ ë¶€ë™ì‚° ëŒ€ì±… íš¨ê³¼ ê¸°ëŒ€", "label": 1},
    {"clean_title": "ì „ì„¸ ì‚¬ê¸° í”¼í•´ì ì†ì¶œ, ë¶€ë™ì‚° ì‹œì¥ ë¶ˆì‹ ", "label": 0},
    {"clean_title": "ì§‘ê°’ í•˜ë½ì„¸ ì§€ì†, íˆ¬ììë“¤ ì†ì‹¤ ìš°ë ¤", "label": 0}
])
df = pd.concat([df, manual], ignore_index=True)

# âœ… ë ˆì´ì•„ì›ƒ 2ì»¬ëŸ¼
col1, col2 = st.columns([1, 1])

with col1:
    st.markdown("### ğŸ“Š ê°ì • ë¶„í¬")
    label_counts = Counter(df["label"])
    full_counts = [label_counts.get(i, 0) for i in range(3)]
    colors = ['#FF6B6B', '#4ECDC4', '#1A535C']
    fig, ax = plt.subplots()
    ax.pie(full_counts, labels=["ë¶€ì •", "ê¸ì •", "ì¤‘ë¦½"], colors=colors, autopct="%.1f%%", startangle=90)
    ax.axis("equal")
    st.pyplot(fig)

with col2:
    st.markdown("### ğŸŒ¤ï¸ ê¸ì • ë‰´ìŠ¤ ì›Œë“œ í´ë¼ìš°ë“œ")
    if df[df["label"] == 1].shape[0] > 0:
        positive_text = " ".join(df[df["label"] == 1]["clean_title"])
        wc = WordCloud(font_path=FONT_PATH, background_color="white", width=500, height=300).generate(positive_text)
        st.image(wc.to_array(), use_column_width=True)
    else:
        st.info("ê¸ì • ë‰´ìŠ¤ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.")

# âœ… ì‚¬ìš©ì ì…ë ¥ ê°ì • ì˜ˆì¸¡
st.markdown("<hr style='border: 1px solid #eee;'>", unsafe_allow_html=True)
st.markdown("### ğŸ§  ì‹¤ì‹œê°„ ë‰´ìŠ¤ ê°ì • ì˜ˆì¸¡")

with st.container():
    user_input = st.text_input("âœï¸ ë¶„ì„í•  ë‰´ìŠ¤ ì œëª©ì„ ì…ë ¥í•˜ì„¸ìš”:", key="sentiment_input")
    if user_input:
        tokens = tokenize(user_input)
        oov_count = sum(1 for word in tokens if word not in tokenizer.word_index)
        oov_ratio = oov_count / len(tokens) if tokens else 0

        if oov_ratio > 0.5:
            st.warning("âš ï¸ ì…ë ¥ëœ ë‹¨ì–´ ëŒ€ë¶€ë¶„ì´ í•™ìŠµë˜ì§€ ì•Šì•„ ì˜ˆì¸¡ ì •í™•ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif oov_ratio > 0:
            st.info(f"â„¹ï¸ ì…ë ¥ì— {oov_count}ê°œì˜ ë¯¸í•™ìŠµ(OOV) ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

        x_input = preprocess(user_input)
        pred = model.predict(x_input)
        label = np.argmax(pred)
        label_text = {0: "ë¶€ì •", 1: "ê¸ì •", 2: "ì¤‘ë¦½"}[label]

        st.success(f"ğŸ¯ ì˜ˆì¸¡ ê°ì • ê²°ê³¼: **{label_text}**")
        st.markdown(f"ì˜ˆì¸¡ í™•ë¥ : `{[round(p, 3) for p in pred[0]]}`")

        new_row = pd.DataFrame([{"clean_title": user_input, "label": label}])
        st.session_state.df = pd.concat([st.session_state.df, new_row], ignore_index=True)
